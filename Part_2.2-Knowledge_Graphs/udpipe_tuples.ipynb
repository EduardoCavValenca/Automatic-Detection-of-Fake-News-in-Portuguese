{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nova regra fiscal: governo vai arrumar a casa ...</td>\n",
       "      <td>política</td>\n",
       "      <td>2023-04-13</td>\n",
       "      <td>https://g1.globo.com/politica/noticia/2023/04/...</td>\n",
       "      <td>a ministra do planejamento e orçamento, simon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pl aciona conselho de ética para acusar deputa...</td>\n",
       "      <td>política</td>\n",
       "      <td>2023-04-13</td>\n",
       "      <td>https://g1.globo.com/politica/noticia/2023/04/...</td>\n",
       "      <td>o pl apresentou nesta quinta–feira (13) ao co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sites chineses usam 3 estratégias para burlar ...</td>\n",
       "      <td>economia</td>\n",
       "      <td>2023-04-13</td>\n",
       "      <td>https://g1.globo.com/economia/noticia/2023/04/...</td>\n",
       "      <td>o anúncio do fim da isenção de imposto para en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subsídio do governo no minha casa, minha vida ...</td>\n",
       "      <td>economia</td>\n",
       "      <td>2023-04-13</td>\n",
       "      <td>https://g1.globo.com/economia/noticia/2023/04/...</td>\n",
       "      <td>o governo publicou uma portaria que estabelec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dólar em queda: saiba se é um bom momento para...</td>\n",
       "      <td>turismo e viagem</td>\n",
       "      <td>2023-04-13</td>\n",
       "      <td>https://g1.globo.com/turismo-e-viagem/noticia/...</td>\n",
       "      <td>na quarta–feira, a moeda estava a r$ 4,9421. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           section   \n",
       "0  nova regra fiscal: governo vai arrumar a casa ...          política  \\\n",
       "1  pl aciona conselho de ética para acusar deputa...          política   \n",
       "2  sites chineses usam 3 estratégias para burlar ...          economia   \n",
       "3  subsídio do governo no minha casa, minha vida ...          economia   \n",
       "4  dólar em queda: saiba se é um bom momento para...  turismo e viagem   \n",
       "\n",
       "         date                                                url   \n",
       "0  2023-04-13  https://g1.globo.com/politica/noticia/2023/04/...  \\\n",
       "1  2023-04-13  https://g1.globo.com/politica/noticia/2023/04/...   \n",
       "2  2023-04-13  https://g1.globo.com/economia/noticia/2023/04/...   \n",
       "3  2023-04-13  https://g1.globo.com/economia/noticia/2023/04/...   \n",
       "4  2023-04-13  https://g1.globo.com/turismo-e-viagem/noticia/...   \n",
       "\n",
       "                                             content  \n",
       "0   a ministra do planejamento e orçamento, simon...  \n",
       "1   o pl apresentou nesta quinta–feira (13) ao co...  \n",
       "2  o anúncio do fim da isenção de imposto para en...  \n",
       "3   o governo publicou uma portaria que estabelec...  \n",
       "4   na quarta–feira, a moeda estava a r$ 4,9421. ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#  Read the g1 dataset.\n",
    "g1 = pd.read_csv('../data/csvs/g1_news_10000.csv')\n",
    "g1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def run_udpipe2_client(text: str):\n",
    "    \"\"\"Run the UDPipe2 client on a text and return the result as a string.\"\"\"\n",
    "    # Create a temporary file with the text.\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\", delete=False) as inputFile:\n",
    "        inputFile.write(text)\n",
    "        inputFile.close()\n",
    "        # Run the UDPipe2 client on the temporary file.\n",
    "        # The output is written to a file.\n",
    "        outputFileName = inputFile.name + \".out\"\n",
    "        subprocess.Popen(f'python3 udpipe2_client.py --model portuguese-gsd-ud-2.10-220711 --input generic_tokenizer --tokenizer ranges --tagger 1 --parser 1 --outfile {outputFileName} {inputFile.name}', shell=True).wait()\n",
    "        \n",
    "        # Read the output file.\n",
    "        with open(outputFileName, \"r\", encoding=\"utf-8\") as outputFile:\n",
    "            result = outputFile.read()\n",
    "        \n",
    "        # Delete the temporary files.\n",
    "        os.remove(inputFile.name)\n",
    "        os.remove(outputFileName)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word, lemma):\n",
    "    return lemma if lemma != '_' else word\n",
    "\n",
    "def joinSubjectOrObject(wordAdjList: list, subjectOrObjectIndex: int, subjectOrObjectWord: str, subjectOrObjectLemma: str, subjectOrObjectXpos):\n",
    "    #   nsubj/obj\n",
    "    #    /      \\\n",
    "    # rel1     rel2\n",
    "    if subjectOrObjectXpos == 'PRON':\n",
    "        return; # ignore pronouns\n",
    "\n",
    "    bannedSubjectOrObjects = {'-', '––', '—', '‘', '’', '“', '”', '♪'}\n",
    "\n",
    "    lemma = get_lemma(subjectOrObjectWord, subjectOrObjectLemma)\n",
    "    if lemma in bannedSubjectOrObjects:\n",
    "        return\n",
    "\n",
    "    relationsToAppend = {'amod', 'nmod', 'nummod', 'case', 'appos', 'flat'}\n",
    "    strComponents = [(subjectOrObjectIndex, lemma)]\n",
    "\n",
    "    def helper(index: int):\n",
    "        nonlocal wordAdjList\n",
    "        nonlocal strComponents\n",
    "        nonlocal relationsToAppend\n",
    "        \n",
    "        for auxIndex, auxWord, auxLemma, _, auxRelation in wordAdjList[index]:\n",
    "            lemma = get_lemma(auxWord, auxLemma)\n",
    "            if auxRelation in relationsToAppend and lemma not in bannedSubjectOrObjects:\n",
    "                strComponents.append((auxIndex, lemma))\n",
    "                helper(auxIndex)\n",
    "    \n",
    "    # O(w), worst case is the tree is a list with all words being of a relation to append\n",
    "    helper(subjectOrObjectIndex)\n",
    "\n",
    "    # O(w * log(w) + len(subj)) to sort the list + join the words\n",
    "    subj = ' '.join([word for _, word in sorted(strComponents, key=lambda x: x[0])])\n",
    "    return subj\n",
    "\n",
    "def joinVerb(wordAdjList: list, verbIndex: int, verbWord: str, verbLemma: str):\n",
    "    #    verb\n",
    "    #    /   \\\n",
    "    #  rel1  verb\n",
    "    #          \\\n",
    "    #         rel2\n",
    "    nonVerbRelationsToAppend = {'advmod'}\n",
    "    verbRelationsToAppend = {'xcomp', 'conj'}\n",
    "\n",
    "    strComponents = [(verbIndex, get_lemma(verbWord, verbLemma))]\n",
    "    def findAllVerb(index: int):\n",
    "        nonlocal wordAdjList\n",
    "        nonlocal verbRelationsToAppend\n",
    "        nonlocal strComponents\n",
    "\n",
    "        for auxIndex, auxWord, auxLemma, auxXpos, relation in wordAdjList[index]:\n",
    "            if auxXpos == 'VERB' and relation in verbRelationsToAppend:\n",
    "                strComponents.append((auxIndex, get_lemma(auxWord, auxLemma)))\n",
    "                findAllVerb(auxIndex)    \n",
    "        \n",
    "    findAllVerb(verbIndex)\n",
    "\n",
    "    # find all non-verb relations to append\n",
    "    verbIndexes = [index for index, _ in strComponents]\n",
    "    for i in verbIndexes:\n",
    "        for auxIndex, auxWord, auxLemma, auxXpos, auxRelation in wordAdjList[i]:\n",
    "            if auxXpos != 'VERB' and auxRelation in nonVerbRelationsToAppend:\n",
    "                strComponents.append((auxIndex, get_lemma(auxWord, auxLemma)))\n",
    "\n",
    "    # O(w * log(w)) to sort the list\n",
    "    verb = [word for _, word in sorted(strComponents, key=lambda x: x[0])]   \n",
    "    verb = ' '.join(verb) \n",
    "\n",
    "    return verb, min(verbIndexes), max(verbIndexes)\n",
    "\n",
    "def parse_udpipe2_output(text: str):\n",
    "    \"\"\"Parse the UDPipe2 output and return a list of tuples.\"\"\"\n",
    "    # get each sentence in a list (they are separated by a blank line)\n",
    "    sentences = text.rstrip().split('\\n\\n')\n",
    "    \n",
    "    tuples = []\n",
    "    for sentence in sentences: # O(n * w^3)\n",
    "        # Remove every line that does not start with a number.\n",
    "        sentence = [line for line in sentence.splitlines() if line and line[0].isdigit()]\n",
    "\n",
    "        # get the word, lemma, xpos, head, and relation (remove lines with ranges as index)\n",
    "        words = [line.split('\\t') for line in sentence]\n",
    "        words_tuples = [(int(w[0]), w[1], w[2], w[4], int(w[6]) if w[6] != '_' else 0, w[7]) for w in words if w[0].isdigit()]\n",
    "\n",
    "        # create a adjlist from the words_tuples\n",
    "        # adjList[i] = every word that has i as head\n",
    "        wordAdjList = [[] for _ in range(len(words_tuples) + 1)]\n",
    "        for index, word, lemma, xpos, head, relation in words_tuples:\n",
    "            wordAdjList[head].append((index, word, lemma, xpos, relation))\n",
    "\n",
    "        # O(w^3 * log(w))\n",
    "        # Form tuples that follow this pattern\n",
    "        #     verb\n",
    "        #    /    \\\n",
    "        #  nsubj   obj\n",
    "        nsubj = ''\n",
    "        verb = ''\n",
    "        obj = ''\n",
    "        for verbIndex, verbWord, verbLemma, verbXpos, _, _ in words_tuples:\n",
    "            if verbXpos != 'VERB': continue\n",
    "            verb, firstVerbIndex, lastVerbIndex = joinVerb(wordAdjList, int(verbIndex), verbWord, verbLemma)\n",
    "            if not verb: continue\n",
    "            for subjIndex, subjWord, subjLemma, subjXpos, subjRelation in wordAdjList[int(firstVerbIndex)]:\n",
    "                if subjRelation != 'nsubj': continue\n",
    "                nsubj = joinSubjectOrObject(wordAdjList, subjIndex, subjWord, subjLemma, subjXpos)\n",
    "                if not nsubj: continue\n",
    "                for objIndex, objWord, objLemma, objXpos, objRelation in wordAdjList[int(lastVerbIndex)]:\n",
    "                    if objRelation != 'obj': continue\n",
    "                    obj = joinSubjectOrObject(wordAdjList, objIndex, objWord, objLemma, objXpos)\n",
    "                    if not obj: continue\n",
    "                    tuples.append((nsubj, verb, obj))\n",
    "\n",
    "            \n",
    "            # reset variables\n",
    "            verb = ''\n",
    "            nsubj = ''\n",
    "            obj = ''\n",
    "\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('governo', 'arrumar cobrar', 'queda de juro'), ('setor produtivo', 'não conseguir mais pegar', 'dinheiro'), ('banco central brasileiro', 'iniciar', 'processo de corte de taxa básico de juro'), ('proposta', 'substituir', 'teto de gasto'), ('pl', 'apresentar', 'pedido de abertura de processo contra deputado federal márcio jerry pcdob– ma'), ('zanatta', 'respeitar', 'deputado'), ('ato de jerry', 'configurar', 'claro prática de importunação sexual'), ('g1', 'procurar', 'deputado'), ('jurídico de legenda', 'também estudar', 'possibilidade jurídico'), ('mesa diretor de câmara', 'ter', 'prazo de três sessão encaminhar'), ('órgão', 'analisar', 'representação contra deputado envolvido em suposto ato de quebra de decoro parlamentar'), ('site chinês', 'usar', '3 estratégia'), ('anúncio de fim de isenção de imposto para encomenda de exterior de us $ 50', 'buscar conter', 'esquema de site de e commerce'), ('cliente', 'pedir', 'produto'), ('site', 'dividir', 'compra'), ('cobrança', 'sempre existir extinguir', 'isenção para comércio entre duas pessoa físico'), ('novo regra', 'extinguir', 'isenção para comércio entre duas pessoa físico'), ('empresa chinês de comércio eletrônico', 'praticar', 'concorrência desleal'), ('empresa chinês', 'praticar', 'concorrência desleal'), ('governo', 'publicar', 'portaria'), ('documento', 'também oficializar', 'meta de programa atender até 2026'), ('subsídio de governo', 'chegar pagar', '5 % de montante'), ('família', 'pagar', '5 % de montante'), ('urbano rural', 'levar', 'benefício de programa'), ('meta atender até 2026', 'levar', 'benefício de programa'), ('especialista', 'além recomendar não comprar', 'valor'), ('banco', 'normalmente possuir além', 'taxa'), ('opção', 'deixar', 'turista'), ('coordenador', 'recomendar', 'uso de cartão de crédito para pequeno gasto como café coisa')]\n"
     ]
    }
   ],
   "source": [
    "# Run the UDPipe2 client on the text.\n",
    "results = []\n",
    "for auxIndex, (title, content) in g1.head()[['title', 'content']].iterrows():\n",
    "    udpipe2_output = run_udpipe2_client(title + '.\\n' + content)\n",
    "        \n",
    "    # Parse the UDPipe2 output.\n",
    "    results.extend(parse_udpipe2_output(udpipe2_output))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the UDPipe2 client on all text using threads\n",
    "import threading\n",
    "from threading import Lock\n",
    "\n",
    "number_of_threads = 24\n",
    "number_of_texts = g1.shape[0]\n",
    "texts_per_thread = number_of_texts // number_of_threads\n",
    "\n",
    "threads = []\n",
    "results = []\n",
    "\n",
    "lock = Lock()\n",
    "def process_texts_thread(lock, start, end):\n",
    "    global results\n",
    "    global g1\n",
    "    partial_results = []\n",
    "    for _, (title, content) in g1.iloc[start:end][['title', 'content']].iterrows():\n",
    "        udpipe2_output = run_udpipe2_client(title + '.\\n' + content)\n",
    "        \n",
    "        # Parse the UDPipe2 output.\n",
    "        partial_results.extend(parse_udpipe2_output(udpipe2_output))\n",
    "\n",
    "    lock.acquire()\n",
    "    results.extend(partial_results)\n",
    "    lock.release()\n",
    "    \n",
    "for i in range(number_of_threads):\n",
    "    start = i * texts_per_thread\n",
    "    end = (i + 1) * texts_per_thread if i < number_of_threads - 1 else number_of_texts\n",
    "    thread = threading.Thread(target=process_texts_thread, args=(lock, start, end))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results in a csv\n",
    "df = pd.DataFrame(results, columns=['subject', 'verb', 'object'])\n",
    "df = df.drop_duplicates()\n",
    "df = df.sort_values(by=['subject', 'verb', 'object'])\n",
    "df.to_csv('results.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
