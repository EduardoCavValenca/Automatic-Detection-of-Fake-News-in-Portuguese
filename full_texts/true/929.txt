Checava se alguém se mataria ao vivo: a rotina de um moderador brasileiro de posts denunciados no Facebook. Na semana em que Zuckerberg anuncia 10 mil novas contratações para moderação de conteúdos, ex-funcionário brasileiro narra desafios da rede contra a violência e jornadas aceleradas para garantir metas de uma revisão a cada 8,5 segundos.  Grupo de amigos incendeia cachorro de rua com isqueiro. Adolescentes são forçados a fazer sexo oral mútuo em acerto de contas do tráfico. Menina com lâmina de barbear anuncia suicídio em vídeo ao vivo. Recém-nascido é espancado por parente no berço. Vaca é despedaçada viva em moedor gigante de madeira. Tudo o que há de pior no Facebook durante 8 horas diárias, de segunda a sexta-feira, em troca de um salário mínimo. O brasileiro Sergio, que pede para não ser identificado, viveu esta rotina por um ano, até abandonar o emprego de revisor de denúncias sobre violência e ódio em português na rede social - e se tornar uma pessoa mais "fria e insensível" na vida offline. "Eu via vídeos ao vivo para checar se alguém se mataria", diz ele, cuja função era decidir o mais rápido possível se publicações agressivas eram toleráveis ou passavam dos limites estabelecidos pelo Facebook. Em seu escritório, a meta para cada revisor era avaliar, por dia, 3.500 fotos, vídeos e textos denunciados. Mais de 7 por minuto, ou um a cada 8,5 segundos. "Impossível não ter erro humano nesse ritmo", diz Sergio, que hoje trabalha como freelancer e decidiu abandonar qualquer interação na rede social depois de a conhecer "por dentro". Na semana em que Mark Zuckerberg anunciou a contratação de 10 mil novos funcionários diretos e indiretos para funções como a desempenhada por Sergio, a rotina do brasileiro em uma espécie de "call center" digital ilustra o desafio enfrentado pelo Facebook para conter milhões de demonstrações explícitas de violência com consequências cada vez mais dramáticas - do incentivo a suicídios infantis a ameaças à segurança nacional do país mais rico do mundo. No Estados Unidos, na semana passada, o Facebook admitiu que grupos russos criaram perfis falsos ligados a movimentos sociais como o Black Lives Matter para divulgar "memes" que incitavam o ódio de grupos racistas durante as eleições de 2016. Segundo as revelações do Senado americano, notícias falsas sobre muçulmanos também teriam sido impulsionadas por russos na rede para irritar o eleitorado xenófobo. Procurada na manhã desta terça-feira 7, a matriz americana do Facebook encaminhou as perguntas da BBC Brasil ao escritório paulista da rede, que no início da noite informou que "não irá comentar" os relatos do ex-funcionário. Nas questões enviadas, a reportagem pedia confirmação das informações sobre carga horária e metas, respostas sobre contratos de trabalho terceirizados e dados sobre eventual apoio psicológico a trabalhadores expostos a publicações violentas. Para além da frieza dos robôs e relatórios, a experiência do brasileiro traz luz ao trabalho daqueles que ocupam funções de base em empresas terceirizadas pelo império de Mark Zuckerberg - com relações de trabalho bem diferentes do clichê de escritórios descolados, do Vale do Silício, com montanhas de M&Ms e cachorros-quentes à disposição das equipes. Em um prédio com longas bancadas de computadores distribuídas em vários andares, Sergio e aproximadamente 500 colegas do mundo inteiro passavam dias avaliando denúncias sobre pedofilia, nudez, necrofilia, suicídios, assassinatos, assédios, ameaças, armas, drogas e violência animal publicadas em 15 idiomas. Segundo o ex-funcionário, nestes centros de revisão da rede social mais usada do planeta, celulares são proibidos, pausas para comida ou banheiro são monitoradas e contratos de trabalho preveem multas e processos judiciais contra vazamento de informações. "Era como um grande call center, sem os telefones. A gente estava ali para atender ao cliente: no caso, o Facebook e todos os seus usuários", diz. Em seu computador, Sergio tinha acesso a uma timeline "alternativa" que exibia apenas as postagens alvo de denúncias de usuários, de forma aleatória, junto a um menu sobre possíveis violações. Os moderadores só visualizam o nome do autor das publicações e não têm acesso a seus perfis completos. Sua missão é apagar, ignorar ou encaminhar a publicação para a avaliação superior - o que ocorre especialmente em casos de suicídio ou pedofilia, que por sua vez são encaminhados a autoridades. As decisões, pautadas por políticas internas da rede social, servem para "educar" os algoritmos, que com o tempo repetem as respostas automaticamente, por meio de recursos avançados de identificação de rostos ou frases ofensivas. "Quanto mais ensinávamos o algoritmo, menos nos tornávamos necessários. Nosso trabalho era tornar o nosso trabalho obsoleto", diz Sergio. O brasileiro também pede sigilo sobre a cidade onde trabalhava, mas conta que o Facebook escolhe locais com alto fluxo de estrangeiros, onde é fácil encontrar pessoas fluentes em diferentes idiomas. Os revisores de conteúdo são normalmente jovens profissionais que vivem no exterior ou que não encontram trabalho em suas áreas. A rotatividade é alta e a maioria não completa um ano no posto. Para conseguirem decidir em menos de 10 segundos se uma publicação merece ou não ser apagada, os funcionários decoram regras e recebem memorandos digitais sobre mudanças em listas de grupos extremistas ou atualizações em políticas, como a que permitiu fotos e vídeos de mães amamentando e imagens de mulheres que passaram por mastectomia. A pressão para cumprir as metas aparecia, segundo Sergio, em reuniões recorrentes com supervisores. "Tinha relatórios periódicos sobre metas de moderação. Os chefes às vezes pareciam cheer-leaders e tentavam nos motivar dizendo que havíamos salvado X pessoas de suicídios ou agressões no mês", conta. "Mas também diziam sempre que a continuidade dos nossos empregos dependia do batimento das metas diárias e citavam outros lugares com resultados melhores que o nosso. A gente nunca sabia quanto tempo o escritório iria durar", completa o ex-funcionário. Há uma semana, ao anunciar um aumento de 47% no faturamento anual do Facebook, que ultrapassou pela primeira vez na história a marca de US$ 10 bilhões em um quadrimestre, Mark Zuckerberg prometeu investir em "pessoas e tecnologia para identificar mau comportamento e remover notícias falsas, discurso de ódio, bullying e outros conteúdos problemáticos" da rede. Em maio, a chefe de Política Global do Facebook, Monika Bickert, comentou o trabalho de revisores como Sergio em um texto sobre os desafios da moderação de conteúdos. "Eles têm um obstáculo: entender o contexto. É difícil julgar a intenção por trás de uma postagem, ou o risco implícito em outra. Alguém publica um vídeo violento de um ataque terrorista. Isso inspirará as pessoas a imitar a violência, ou a falar contra isso? Alguém escreve uma piada sobre suicídio. É um mero comentário ou um grito de ajuda?" O brasileiro confirma as dificuldades, mas diz que não conseguia discutir decisões com superiores. "Não tinha espaço para pensamento critico, o trabalho tinha que ser automático e acelerado. Era seguir o manual, apertar botão e não fazer muitas perguntas", diz. Os dados mais recentes do Facebook apontam que a rede exclui quase 300 mil publicações por mês. Entre os brasileiros, segundo o ex-revisor, boa parte dos conteúdos denunciados são casos de nudez. "Ver conteúdos fortes todos os dias te faz perder a sensibilidade para certas coisas. Especialmente em relação à nudez - eram tantas selfies de gente nua, closes em pênis, vaginas e mamilos, que a pornografia perdeu a graça", afirma. Como não há consenso nas leis de diferentes países sobre conteúdos ofensivos ou discurso de ódio online (frases que podem levar à prisão em locais como a Alemanha podem ser protegidas como liberdade de expressão pela Constituição dos EUA), o Facebook criou suas próprias regras, considerando raça e etnia, religião, gênero e orientação sexual como "categorias protegidas". Os revisores são orientados, seguindo as normas da rede, a apagar "qualquer ataque direto a pessoas" baseado com base nestas categorias. Sobre violência, a rede determina que imagens de interesse público, "como abusos de direitos humanos ou atos de terrorismo", sejam mantidas quando as postagens expressarem reprovação ou conscientização. Em casos de compartilhamento "pelo prazer sádico de celebrar ou enaltecer a violência", elas devem ser apagadas. Muitos casos geravam discórdia entre os funcionários. "Ferimento grave, por exemplo, podia. Se depois de ser atropelada em um vídeo a pessoa ainda estivesse se mexendo, o conteúdo era marcado como "sensível", mas continuava no ar", lembra. "Só se houvesse morte a gente apagava". Além da pressa, códigos sociais usados por diferentes grupos podiam dificultar o trabalho dos moderadores. "Muitos gays chamam uns aos outros de veado, por exemplo. Veado (ou faggot, em inglês) era uma palavra proibida, a não ser que fosse uma autorrefêrencia. E acontecia sempre de um gay usar a palavra veado e ter a conta bloqueada ou desativada por ofensa", diz Sergio. "Muitas vezes, na moderação, era a gente quem decidia se a pessoa era gay ou não na hora de analisar a denúncia." A rede social costuma reconhecer falhas no sistema. "Nossos erros causaram muita preocupação em várias comunidades, inclusive entre grupos que sentem que nós agimos - ou deixamos de agir - por parcialidade", dizia um post do Facebook em julho. "Estamos profundamente empenhados em discutir e enfrentar a parcialidade em qualquer lugar do mundo. Ao mesmo tempo, trabalhamos para corrigir nossos erros rapidamente, assim que eles acontecem." Sergio diz que nunca encontrou uma denúncia de postagem envolvendo algum amigo ou conhecido - as publicacões para revisão apareciam em seu computador de forma aleatória. Ameaças contra políticos e pessoas públicas, no entanto, eram frequentes, diz ele - e as regras da rede para este público são distintas. "Nós apagamos ameaças reais contra a integridade física das pessoas e removemos ameças específicas de roubo, vandalismo ou prejuízo financeiro", dizem as regras oficiais do Facebook. O texto oficial diz, porém, que em casos de "pessoas com visibilidade pública", as chances reais "das ameaças se concretizarem" são levadas em consideração na hora de apagar ou não a postagem. Segundo Sergio, publicações como o vídeo em que o deputado Jair Bolsonaro afirma que a colega Maria do Rosario "não merece ser estuprada" e comentários violentos continuaram no ar porque a deputada se encaixaria nesta categoria. "Qualquer pessoa com mais de 100 mil seguidores é considerada pública", afirma o brasileiro. Ele conta que o que mais chocava os colegas era a crueldade de publicações com agressões a animais. "Num deles aparecia uma máquina de matadouro, com uma corda que girava presa a um motor e tinha uma vaca amarrada em uma das pontas. A corda a puxava e ela era despedaçada viva", diz. A exposição excessiva a imagens violentas já levou moderadores do Facebook a desenvolverem distúrbios de ansiedade, problemas sexuais e pânico, segundo jornais estrangeiros. O brasileiro, no entanto, diz que era afetado em menor grau pelas imagens. "Primeiro, porque nasci no Brasil, e nossas referências sobre violência tendem a ser mais agressivas que as de colegas europeus, por exemplo", afirma. Ele continua: "Depois, porque eu via todos os dias vídeos de brutalidades contra crianças, minorias e bichos, e a gente acaba se acostumando com imagens gráficas. Para mim, a crueldade humana em termos de palavras, apoiando agressões, pregando ódio, rindo de vítimas em comentários e compartilhamentos, era sempre muito pior". O perfil pessoal de Sergio no Facebook permanece abandonado desde a época da demissão.